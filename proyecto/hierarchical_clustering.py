from scipy.cluster.hierarchy import dendrogram
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
import numpy as np
import random
from scipy.spatial import distance
import pickle

from sklearn.metrics import silhouette_score

def help():
    print("########################Ayuda para el uso de la clase hierarchical_clustering####################################################")
    print("#1.-Cuando se llama a la constructora se le dan los vectores, el grado para la distancia minkowski y el tipo de distancia intergrupal")
    print("#    ejemplo : hc = hierarchical_clustering(vectors, distance_type,p=3)  #siendo distance_type 'average','mean','complete' o 'single'")
    print("#2.-Para iniciar la clusterización se llama al metodo cluster de esa misma clase")
    print("#    ejemplo : proc = hc.cluster()")
    print("#Este metodo devuelve un objeto de la clase procesarCluster, que como su nombre indica es la clase que se encarga de analizar el arbol generado por la clusterizacion")
    print("#3.-Si queremos, por ejemplo, obtener 4 clusters con una distancia maxima de 20 se podría expresar tal que así")
    print("#    ejemplo : proc.buscar_nodos(num_clusters=4,dist_max=10)")
    print("#4.-Esta clase también se encarga de hacer las predicciones, tiene dos metodos para esto, predict y predict_multiple, el primero devuelve el lavel de un vector, y el segundo devuelve el de varios vectores en un diccionario")
    print("#    ejemplo :  labels=proc.predict_multiple(test)")
    print("#En ese último caso se devolverian los labels del test que contendría un array de vectores")
    print("#5.-Finalmente, se podrá visualizar el arbol generado en la clase hierarchical clustering mediante el metodo draw_dendrogram()")
    print("#    ejemplo : hc.draw_dendrogram()")
    print("#################################################################################################################################")
    print()
    print("--------------------------------English----------------------------------------------")
    print()
    print("########################Help for Using the hierarchical_clustering Class########################################################")
    print("#When calling the constructor, you need to provide the vectors, the Minkowski distance degree, and the intergroup distance type")
    print("#    example: hc = hierarchical_clustering(vectors, distance_type, p=3)#being distance_type 'average','mean','complete' or 'single'")
    print("#To initiate the clustering, you call the cluster method of the same class")
    print("#    example: proc = hc.cluster()")
    print("#This method returns an object of the procesarCluster class, which, as the name suggests, is responsible for analyzing the tree generated by clustering")
    print("#If, for instance, you want to obtain 4 clusters with a maximum distance of 20, you could express it like this")
    print("#    example: proc.find_nodes(num_clusters=4, dist_max=20)")
    print("#This class also handles predictions, with two methods for this purpose: predict and predict_multiple. The former returns the label for a single vector, and the latter returns labels for multiple vectors in a dictionary")
    print("#    example: labels = proc.predict_multiple(test)")
    print("#In the latter case, it would return labels for the 'test' which would contain an array of vectors")
    print("#Finally, you can visualize the tree generated in the hierarchical clustering class using the draw_dendrogram method")
    print("#    example: hc.draw_dendrogram()")
    print("#################################################################################################################################")
    
class procesarCluster():
    def __init__(self,vectors,arbol,num_clusters=4,dist_max=20,distance_type='single',p=2):
        self.distance_type=distance_type
        self.vectors=vectors
        self.tree=arbol
        self.num_clusters=num_clusters
        self.dist_max=dist_max
        self.nodoPadre=max(self.tree.keys())
        self.clusters=[]
        self.grado=p
        self.centroides={}
    def obtener_nodos_finales(self, nodo):
        #Recorre el arbol desde un nodo para obtener los indices de los vectores
        if self.tree[nodo]['hijo1'] is None and self.tree[nodo]['hijo2'] is None:
            #print(nodo)
            return [nodo]
        else:
            nodos_finales = []
            if self.tree[nodo]['hijo1'] is not None:
                nodos_finales.extend(self.obtener_nodos_finales(self.tree[nodo]['hijo1']))
            if self.tree[nodo]['hijo2'] is not None:
                nodos_finales.extend(self.obtener_nodos_finales(self.tree[nodo]['hijo2']))
            return nodos_finales
    def draw_dendrogram(self):
        # Obtener las distancias y las uniones
      
        linkage=[]
        for clave in sorted(self.tree.keys(), reverse=True):
            #print(clave)
            if(self.tree[clave]['hijo1']is not None):
                linkage.append([int(self.tree[clave]['hijo1']),int(self.tree[clave]['hijo2']),float(self.tree[clave]['distancia']),int(len(self.obtener_nodos_finales(clave)))])
       
        linkage=linkage[::-1] #invertir el orden de la array
        print(linkage)
       
    
        # Crear el dendrograma
        plt.figure(figsize=(10, 5))
       
       
        dendrogram(linkage,labels=list(range(len(linkage)+1)), leaf_rotation=90, leaf_font_size=8, orientation='top')
        plt.xlabel('Índices de los clusters')
        plt.ylabel('Distancia')
        plt.title('Dendrograma '+'Distancia:'+self.distance_type)
        plt.show()
    def predict(self,vector):
        #Pre: Se da un vector
        #Post: Devuelve el label de a que cluster pertenece
        distancia=99999
        nodo=None
        for x in self.centroides.keys():
            nueva_dist = distance.minkowski(vector, self.centroides[x], p=self.grado)
            #nueva_dist = np.linalg.norm(np.array(vector) - np.array(self.centroides[x]))
            if (nueva_dist<distancia):
                distancia=nueva_dist
                nodo=x
                label=x
        print("El punto:"+" pertenece al cluster:"+str(nodo)+" con una distancia de:"+str(distancia))
        return(label)
    def predict_multiple(self,vectors):
        #Pre: Se da una lista de vectores
        #Post: Devuelve un diccionario que asocia cada indice de la lista de vectores con el cluster al que pertenece
        labels={}
        i=0
        for x in vectors:
            labels[i]=self.predict(x)
            i+=1
        return(labels)
    def añadir_linkage(self,linkage,nodo):
        if(self.tree[nodo]['hijo1'] is not None):
            linkage.append([int(self.tree[nodo]['hijo1']),int(self.tree[nodo]['hijo2']),float(self.tree[nodo]['distancia']),int(len(self.obtener_nodos_finales(nodo)))])
            linkage=self.añadir_linkage(linkage,(self.tree[nodo]['hijo1']))
            linkage=self.añadir_linkage(linkage,(self.tree[nodo]['hijo2']))
        return(linkage)
   
    
    def calcular_centroide(self,indice):
        #Pre : Dado un indice de un nodo del arbol
        #Post : Devuelve el centroide de sus vectores
        cluster=self.obtener_nodos_finales(indice)
        clusters=[]
        for x in cluster:
            clusters.append(self.vectors[x])
        cluster=clusters
        num_vectores = len(cluster)
        num_caracteristicas = len(cluster[0])  # Suponemos que todos los vectores tienen la misma longitud

        centroide = [0] * num_caracteristicas

        for vector in cluster:
            for i in range(num_caracteristicas):
                centroide[i] += vector[i] / num_vectores

        self.centroides[indice]=centroide
    def cortar_arbol(self,num_clusters=4,dist_max=20):
        #Pre: Se da el número de clusters objetivo o la distancia maxima objetivo
        #Post: Devuelve los nodos en los que se cumplen los requisitos.
        self.centroides={}
        self.dist_max=dist_max
        self.num_clusters=num_clusters
        self.clusters=[self.nodoPadre]
        hemosLlegado=False
        while len(self.clusters)<num_clusters and not hemosLlegado:
            dist=0
            siguiente_nodo=None
            print(self.clusters)
            for x in self.clusters:
                if self.tree[x]['distancia']>dist and self.tree[x]['distancia']> self.dist_max:
                    dist=self.tree[x]['distancia']
                    siguiente_nodo=x
            print("El siguiente nodo:"+str(siguiente_nodo))
            if(siguiente_nodo) is None:
                hemosLlegado=True
            else:
                self.clusters.remove(siguiente_nodo)
                self.clusters.append(self.tree[siguiente_nodo]['hijo1'])
                self.clusters.append(self.tree[siguiente_nodo]['hijo2'])
                
            #print(self.clusters)
        
        for x in self.clusters:
            print("El cluster "+str(x)+" contiene estos vectores:")
            vectores=[]
            for y in self.obtener_nodos_finales(x):
                vectores.append(self.vectors[y])
            self.calcular_centroide(x)     
        print("Los centroides son:"+str(self.centroides))
        #self.draw_dendrogram()

    def save(self):
        #Funcion para guardar el modelo
        filename = f"{self.distance_type}_{self.num_clusters}.pkl"
        with open(filename, 'wb') as file:
            pickle.dump(self, file)
        print(f"Guardado en {filename}")
    
    def dict_to_array(self, dict_labels, num_samples):
        labels = [dict_labels.get(i, -1) for i in range(num_samples)]
        return labels

    def silhouette_score(self, num_clusters, dist_max):
        self.cortar_arbol(num_clusters=num_clusters, dist_max=dist_max)
        print("NUM CLUSTERS")
        print(len(self.clusters))
        X = []
        Y = []
        for c in self.clusters:
            indices = self.obtener_nodos_finales(c)
            instancias = [self.vectors[i] for i in indices]
            X.extend(instancias)
            Y.extend([c] * len(instancias))
        
        # Calcula la puntuación de la silueta
        return silhouette_score(X, Y, metric='euclidean')
   def metrics_evaluation(self, X_train, y_train, pred_labels, true_labels):
        input("Metricas externas de evaluación: Cluster vs Class Evaluation")
        pred_labels = [0, 0, 1, 1, 2, 2]
        true_labels = [0, 0, 1, 2, 2, 2]
        self.metricas_externas(true_labels, pred_labels)
        
        input("Metricas internas de evaluación: Evaluación de la estructura intrínseca de los datos")
        self.dist_cofonetica()
        for c in range(1,15,1):
            self.metricas_internas(c)
        
        
    def metricas_externas(self, true_labels, pred_labels):
        # El ínidice de Jaccard: mide la similitud entre los conjustos de elementos dentro de los clusteres y las clases reales. 1 el mejor
        # Indíce Rand: compara la simlitud entre pares de elementos del mismo cluster y clase real. Cuanto más alto mejor concordancia entre el clustering y las etiquetas reales
        jaccard = jaccard_score(true_labels, pred_labels, average='weighted')
        adjusted_rand = adjusted_rand_score(true_labels, pred_labels)
        rand = rand_score(true_labels, pred_labels)
        
        print(f'Jackar index con un valor de {jaccard}')
        print(f'Adjusted rand index con un valor de {adjusted_rand}')
        print(f'Rand index con un valor de {rand}')
        
        N11, N10, N01, N00 = self.pairwise_comparision(true_labels, pred_labels)
        print("N11: ", N11)
        print("N10: ", N10)
        print("N01: ", N01)
        print("N00: ", N00)
        
        data = [[None, 'Mismo grupo', 'Diferente Grupo'], 
                ['Mismo Cluster', N11, N10], 
                ['Diferente Cluster', N01, N00]]
        
        fig, ax = plt.subplots()
        ax.axis('equal')
        ax.axis('off')
        table = ax.table(cellText=data, cellLoc='center', loc='center', cellColours=[['palegreen']*3]*3)
        table.auto_set_font_size(False)
        table.set_fontsize(12)
        table.scale(1,2)
        
        plt.show()
        
        # Calculo sin utilizar librerias
        
        jaccard = N11 / (N11 + N10 + N01)
        rand = (N11+ N00) / (N11 + N10 + N01 + N00)
        folkes_malow = sqrt((N11/(N11 + N01))*(N11/(N11 + N10)))
        
        print(f'Jackar index con un valor de {jaccard}')
        print(f'Rand index con un valor de {rand}')
        print(f'Folkes Malow index con un valor de {folkes_malow}')
        
        return 
    
    def metricas_internas(self, n_clusts, y_train, X_train, labels):
        self.silueta_eval(n_clusts)
        self.cortar_arbol(num_clusters=n_clusts, dist_max=0)
        X = []
        Y = []
        for c in self.clusters:
            indices = self.obtener_nodos_finales(c)
            instancias = [self.vectors[i] for i in indices]
            X.extend(instancias)
            Y.extend([c] * len(instancias))
        # Calculo de Davies Bouldin 
        davies_bouldin = davies_bouldin_score(X_train, labels)
        # Calculo de homogeneidad de los clusters
        homo_clusts = homogeneity_score(y_train, labels)
        
        return

    def pairwise_comparision(self, part_C, part_G):
        n_samples = len(part_C)
        N11,N10, N01, N00 = 0, 0, 0, 0
        
        for i in range(n_samples):
            for j in range(i+1, n_samples):
                if part_C[i] == part_C[j] and part_G[i] == part_G[j]:
                    N11 += 1
                elif part_C[i] == part_C[j] and part_G[i] != part_G[j]:
                    N10 += 1
                elif part_C[i] != part_C[j] and part_G[i] == part_G[j]:
                    N01 += 1
                elif part_C[i] != part_C[j] and part_G[i] != part_G[j]:
                    N00 += 1

        return N11, N10, N01, N00
    
    
    def silueta_eval(self, c):
        input("evaluacion de silueta")
        resultados = {}
        siluetas = {}
        #n_dims = list(range(1, max(len(vec) for vec in self.vectors) + 1))
        #dists = self.rango_de_distancias(self.vectors)
        X = self.cortar_arbol(num_clusters=c, dist_max=0)
        X_fit = np.array(X)
        print(X_fit)
        #nodos_X = self.obtener_nodos_finales(self.nodoPadre)
        #print(nodos_X)
        input("silueta calculo")
        # Obtenemos las etiquetas
        labels_dic = self.predict_multiple(X_fit)
        labels = self.dict_to_array(labels_dic ,len(X_fit))
        print(labels)
        input("labels")
        silueta = 0
        if len(np.unique(labels))>1:
            silueta = silhouette_score(X, labels, metric='euclidean')  # Calcula la puntuación de la silueta
        # Almacena los resultados en un diccionario
        resultado = {
            'clusters': c,
            'silueta': silueta
        }
        resultados[c] = resultado

        # Después de recorrer todas las combinaciones, imprime o utiliza los resultados y siluetas
        print("Resultados:", resultados)
        self.graficar_siluetas(siluetas)
    
    def graficar_siluetas(self, siluetas):
            # Representamos los resultados en una gráfica de barras
            
            fig, ax = plt.subplots(figsize=(5,2))
            ax.set_xlabel('label')
            ax.set_ylabel('Silhouette score')

            plt.bar(siluetas.keys(), siluetas.values(), align='center', color='#007acc')
            input("graficando siluetas")
            
    def dist_cofonetica(self):
        # Esta métrica mide cuán bien la jerarquía de clustering representa la estructura original de tus datos
        # Mide la correlación entre las distancias cophenéticas (distancias entre los elementos originales) 
        # y las distancias en la jerarquía de clustering
        linkage=[]
        for clave in sorted(self.tree.keys(), reverse=True):
            #print(clave)
            if(self.tree[clave]['hijo1']is not None):
                linkage.append([int(self.tree[clave]['hijo1']),int(self.tree[clave]['hijo2']),float(self.tree[clave]['distancia']),int(len(self.obtener_nodos_finales(clave)))])
       
        linkage=linkage[::-1] #invertir el orden de la array
        print(linkage)
        
        #print(squareform(cophenet(linkage, pdist(self.vectors))))
        c, coph_dists = cophenet(linkage, pdist(self.vectors))
        print(c)
        print(coph_dists)
        # c es la distancia de correlación cofenética
        # coph_dists contiene las distancias cophenéticas entre tus datos, 
        # estos sonla representación jerárquica de las distancias entre los elementos originales
        input("cofonetica")    
    
    def graph(ax, pca_t, color='blue'):
        PC_values = np.arange(pca_t.n_components_) + 1
        ax.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color=color)
    
    def graficar_dimensionalidad(self):
        self.cortar_arbol(num_clusters=8, dist_max=0)
        dim = max(len(vec) for vec in self.vectors)
        pca = PCA(n_components=dim)
        pca.fit(self.vectors)
    
        # simplemente graficar
        fig, (ax1) = plt.subplots(1)
        fig.suptittle('Variación de datos según la dimension')
        graph(ax1, pca)
    def dimension_mas_optima(self):
        # Inicializa el PCA y ajusta los datos
        pca = PCA()
        pca.fit(train_corpus)

        # Calcula la varianza explicada acumulada
        cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)

        # Encuentra el número de componentes que preservan al menos el 95% de la varianza
        n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1

        # Encuentra el número de componentes que preservan al menos el 99% de la varianza
        n_components_99 = np.argmax(cumulative_variance_ratio >= 0.99) + 1

        print(f'Número de componentes para el 95% de varianza acumulada: {n_components_95}')
        print(f'Número de componentes para el 99% de varianza acumulada: {n_components_99}')

        print('Dim originally: ',X_train.shape)
        # Reducir las dimensiones para visualizarlas: PCA
        pca = PCA(n_components=n_components_95)
        pca.fit(X_train)
        # Cambio de base a dos dimensiones PCA
        X_train_PCAspace = pca.transform(X_train)
        print('Dim after PCA: ',X_train_PCAspace.shape)   

    def graficacion_siluetas(self):
        rango_n_clusts = list(range(2,6,1))
        for num_clusters in rango_n_clusts:
            # creando el plot
            fig, (ax1, ax2) = plt.subplots(1, 2)
            fig.set_size_inches(18, 7)
            
            # El 1er subplot es el plot del silhouette
            # este puede ir de -1, 1 but in this example all
            ax1.set_xlim([-0.1, 1])
            # añadir separacion entre las siluetas individuales de cada cluster
            ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])
            print("NUM CLUSTERS")
            self.cortar_arbol(num_clusters=num_clusters, dist_max=0)
            X = []
            Y = []
            for c in self.clusters:
                indices = self.obtener_nodos_finales(c)
                instancias = [self.vectors[i] for i in indices]
                X.extend(instancias)
                Y.extend([c] * len(instancias))
            siluet = silhouette_score(X, Y, metric='euclidean')
            print("For n_clusters =",n_clusters, "La media silhouette_score es :",
            siluet,)
            
            y_lower = 10
            for i in range(n_clusters):
                # Aggregate the silhouette scores for samples belonging to
                # cluster i, and sort them
                ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]

                ith_cluster_silhouette_values.sort()

                size_cluster_i = ith_cluster_silhouette_values.shape[0]
                y_upper = y_lower + size_cluster_i

                color = cm.nipy_spectral(float(i) / n_clusters)
                ax1.fill_betweenx(
                    np.arange(y_lower, y_upper),
                    0,
                    ith_cluster_silhouette_values,
                    facecolor=color,
                    edgecolor=color,
                    alpha=0.7,
                )    
class hierarchical_clustering:
    def __init__(self, vectors, inter_distance_type,p=2):
        #CONSTRUCTORA DE LA CLASE DE ENTRENAMIENTO
        self.iters=0
        self.grado=p
        self.vectors = vectors
        self.tree={}
        self.centroides={}
        self.distance_type = inter_distance_type
        if (self.distance_type=="average"):
            for i in range(len(vectors)):
             self.centroides[i]=self.vectors[i]
        self.clusters = [i for i in range(len(vectors))]
        
        #self.distances = [[self.distance(vectors[i], vectors[j]) for j in range(len(vectors))] for i in range(len(vectors))]
    @classmethod
    def load(cls, filename):
        #Función para cargar el modelo
        with open(filename, 'rb') as file:
            obj = pickle.load(file)
        print(f"Cargado desde {filename}")
        return obj
    def calcular_centroide(self,indice):
        #Pre:Dado un indice obtiene todos los vectores que contiene
        #Post:Devuelve el centroide asociado a los vectores pertenecientes al cluster
        cluster=self.clusters_ind[indice]
        clusters=[]
        for x in cluster:
            clusters.append(self.vectors[x])
        cluster=clusters
        num_vectores = len(cluster)
        num_caracteristicas = len(cluster[0])  # Suponemos que todos los vectores tienen la misma longitud

        centroide = [0] * num_caracteristicas

        for vector in cluster:
            for i in range(num_caracteristicas):
                centroide[i] += vector[i] / num_vectores

        self.centroides[indice]=centroide
    def calcular_distancias_NuevoNodo(self,num_nodos):
        #Pre:Se le aporta el nodo recien creado
        #Post:Actualiza la distancia al resto de clusters posibles
        distancia=9999
        for x in self.clusters:
            #print("Se esta recalculando distancia de "+ str(x)+" y " + str(num_nodos))
            if(self.distance_type!="average"):
                distancia=self.distancia_intracluster(self.clusters_ind[num_nodos],self.clusters_ind[x])
            self.distancias[num_nodos,x]=distancia
            self.distancias[x,num_nodos]=distancia
        return
    def actualizar_arbol(self, nodo1, nodo2, distancia):
        #Pre:Dado los hijos y la distancia actualiza el arbol
        #Post:El arbol se verá incrementado con un nuevo nodo
        num_nodos = len(self.vectors)+self.iters 
        nueva_distancia =  0
        self.clusters_ind[num_nodos] = self.clusters_ind.get(nodo1, []) + self.clusters_ind.get(nodo2, [])
        print(self.clusters_ind[num_nodos])
        self.clusters.remove(nodo1)
        self.clusters.remove(nodo2)
        self.calcular_distancias_NuevoNodo(num_nodos)
        self.clusters.append(num_nodos)
        
        
        # Crear el nuevo nodo
        nuevo_nodo = {
            'hijo1': nodo1,
            'hijo2': nodo2,
            'distancia': distancia,
            'iteracion': self.iters #ESTO PARA NUM CLUSTERS!!
            #'padre':None,
            #'vector': None  # Aquí deberías definir el vector del nuevo nodo si es necesario
        }
        print("nuevo nodo" + str(nuevo_nodo))
        # Añadir el nuevo nodo al árbol
        self.tree[num_nodos] = nuevo_nodo
        if(self.distance_type=="average"):
            self.centroides.pop(nodo1)
            self.centroides.pop(nodo2)
            self.calcular_centroide(num_nodos)

        # Incrementar el contador de iteraciones

        return num_nodos

    def generar_diccionario(self):
        #Genera la base del arbol puesto que esto dará el indice para acceder a los vectores
        resultado = {}
        
        for i, array in enumerate(self.vectors):
            id_unico = i  
            resultado[id_unico] = {'hijo1': None, 'hijo2': None, 'distancia': 0, 'iteracion': -1}
        return resultado
    
    def generar_distancias(self):
        #Pre dado los vectores
        #Post genera las tuplas de distancias entre todos los puntos para evitar recalcularlas
        n = len(self.vectors)
        vectores=[np.array(vec) for vec in self.vectors]
        distancias = {}
        self.clusters_ind={}
        for i in range(n):
            self.clusters_ind[i]=[i]
            for j in range(i+1, n):
                distancia =  distance.minkowski(vectores[i],vectores[j],self.grado)
                #distancia = np.linalg.norm(vectores[i] - vectores[j])
                distancias[(i, j)] = distancia
                distancias[(j, i)] = distancia
        return distancias
    
    ##Aqui se hacen las distancias
    def mean_link(self,cluster1, cluster2):
        #Calcula la distancia mean
        total_distancia = 0
        num_pares = 0
        for punto1 in cluster1:
            for punto2 in cluster2:
                total_distancia += self.distancias[(punto1, punto2)]
                num_pares += 1
        return total_distancia / num_pares
    
    def single_link(self,cluster1, cluster2):
        #Calcula la distancia single
        min_distancia = float('inf')
        for punto1 in cluster1:
            for punto2 in cluster2:
                distancia = self.distancias[(punto1, punto2)]
                if distancia < min_distancia:
                    min_distancia = distancia
        return min_distancia
    
    def complete_link(self,cluster1, cluster2):
        #Calcula la distancia complete
        max_distancia = 0
        for punto1 in cluster1:
            for punto2 in cluster2:
                distancia = self.distancias[(punto1, punto2)]
                if distancia > max_distancia:
                    max_distancia = distancia
        return max_distancia
    
    def average_link(self,cluster1, cluster2):
        #Calcula la distancia avg
        total_distancia = 0
        num_pares = 0
        for punto1 in cluster1:
            for punto2 in cluster2:
                total_distancia += self.distancias[(punto1, punto2)]
                num_pares += 1
        return total_distancia / num_pares
    def mean_link(self):
        centroides = list(self.centroides.keys())
        coordenadas = list(self.centroides.values())
        
        # Calcula todas las distancias entre pares de centroides
        
        distancia_minima = float('inf')
        centroides_mas_cercanos = (None, None)
        
        # Encuentra los centroides más cercanos
        for i in range(len(centroides)):
            for j in range(i + 1, len(centroides)):
                distancia_actual = np.linalg.norm(np.array(coordenadas[i]) - np.array(coordenadas[j]))
                if distancia_actual < distancia_minima:
                    distancia_minima = distancia_actual
                    centroides_mas_cercanos = (centroides[i], centroides[j])
                    centroide1=centroides[i]
                    centroide2=centroides[j]
        
    
        return centroide1,centroide2, distancia_minima
        

    def distancia_intracluster(self,ind_vect_cluster1,ind_vect_cluster2):
        #Redirige para saber que tipo de disttancia intracluster usar
        #'single','complete','average','mean'
        if(self.distance_type=="single"):
            distancia=self.single_link(ind_vect_cluster1,ind_vect_cluster2)
        elif(self.distance_type=="mean"):
            distancia=self.average_link(ind_vect_cluster1,ind_vect_cluster2)
        elif(self.distance_type=="average"):
            #distancia=self.mean_link(ind_vect_cluster1,ind_vect_cluster2)

            print("Esto no se deberia ejecutar nunca puesto que se trata en otro momento")
        else:
            distancia=self.complete_link(ind_vect_cluster1,ind_vect_cluster2)
        #print("Distancia entre cluster:"+str(ind_vect_cluster1)+" y cluster:"+str(ind_vect_cluster2)+" :"+str(distancia))
        
        return(distancia)
    
    def encontrar_clusters_mas_cercanos(self):
        #Pre: Tenemos una lista de cluster mayor que 1
        #Post: Devuelve los dos nodos con distancia intergrupal menor para posteriormente unirlos llamando a la función añadir_arbol
        if(self.distance_type=="average"):
           indice_cluster1, indice_cluster2,distancia_minima=self.mean_link()
        else:
            distancia_minima = float('inf')
            indice_cluster1 = None
            indice_cluster2 = None
                
            for i in range(len(self.clusters)):
                cluster1 = self.clusters[i]
                vectores_cluster1=self.clusters_ind[cluster1]
                for j in range(i + 1, len(self.clusters)):
                    
                    cluster2 = self.clusters[j]
                    
                    if ((self.clusters[i],self.clusters[j]) not in self.distancias):
                        
                        vectores_cluster2=self.clusters_ind[cluster2]
                        
                        distancia = self.distancia_intracluster(vectores_cluster1, vectores_cluster2)
                    else:
                        #print("Distancia precalculada")
                        distancia=self.distancias[(self.clusters[i],self.clusters[j])]
                    
                    if distancia < distancia_minima:
                        distancia_minima = distancia
                        indice_cluster1 = cluster1
                        indice_cluster2 = cluster2
                    elif distancia == distancia_minima:
                        if random.choice([True, False]):
                            indice_cluster1 = cluster1
                            indice_cluster2 = cluster2
            print("Los nodos más cercanos son:"+str(indice_cluster1)+" y "+str(indice_cluster2))
            
        return indice_cluster1, indice_cluster2,distancia_minima

         
    def cluster(self):
        #Funcion que inicia el entrenamiento, dado una lista de vectores esta se ira agrupando hasta quedar solo 1 un cluster, se verá reflejado en el arbol y podra recorrerse con una clase auxiliar
        if self.iters == 0:
            self.tree = self.generar_diccionario()
            self.distancias=self.generar_distancias()
              
        while len(self.clusters) > 1:
            #nodo1, nodo2 = self.encontrar_nodos_mas_cercanos()
            #self.actualizar_arbol(nodo1, nodo2)
            nodo1,nodo2,distancia=self.encontrar_clusters_mas_cercanos()
            self.actualizar_arbol(nodo1,nodo2,distancia)
            self.iters += 1
        proc=procesarCluster(self.vectors,self.tree,distance_type=self.distance_type,p=2)
        
        return proc

    @staticmethod
    def load(filename):
        with open(filename, 'rb') as file:
            return pickle.load(file)


if __name__ == "__main__":
    help() 
